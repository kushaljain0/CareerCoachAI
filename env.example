# LLM Configuration (Gemini is the default)
GOOGLE_API_KEY=your_google_api_key_here
OPENAI_API_KEY=your_openai_api_key_here  # Optional backup

# LLM Model Selection
DEFAULT_LLM=gemini  # Options: gemini, openai, claude
GEMINI_MODEL=gemini-2.5-flash  # Options: gemini-1.5-flash, gemini-1.5-pro, gemini-2.0-flash-exp, gemini-2.0-pro-exp, gemini-2.5-pro
OPENAI_MODEL=gpt-4o-mini

# LLM Response Configuration
DEFAULT_MAX_TOKENS=1000  # Default max tokens for LLM responses
INTENT_ANALYSIS_MAX_TOKENS=100  # Max tokens for intent analysis
CHAT_RAG_TOP_K=3  # Number of RAG results to retrieve for chat responses

# RAG Configuration
DATA_DIR=./data
VECTOR_DB_PATH=./data/vector_db
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
EMBEDDING_MODEL=all-MiniLM-L6-v2
TOP_K_RESULTS=5

# MCP Server Configuration
MCP_SERVER_HOST=0.0.0.0
MCP_SERVER_PORT=8000

# Frontend Configuration
VITE_API_BASE_URL=http://localhost:8000

# Logging
LOG_LEVEL=INFO 